{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxkucher/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/maxkucher/pytorch/tour-de-room/room_NN.pickle\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    analyzer_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7)                 70        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 32        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245 (980.00 Byte)\n",
      "Trainable params: 245 (980.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "analyzer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 bottles, 1 bed, 192.9ms\n",
      "Speed: 2.6ms preprocess, 192.9ms inference, 761.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "\n",
      "0: 384x640 2 bottles, 1 bed, 97.9ms\n",
      "Speed: 34.3ms preprocess, 97.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bottle, 1 cup, 1 bed, 104.4ms\n",
      "Speed: 1.3ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bottle, 1 cup, 1 bed, 115.1ms\n",
      "Speed: 1.4ms preprocess, 115.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 2 bottles, 1 bed, 235.9ms\n",
      "Speed: 1.8ms preprocess, 235.9ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "\n",
      "0: 384x640 3 bottles, 1 bed, 307.6ms\n",
      "Speed: 38.0ms preprocess, 307.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 2 bottles, 1 bed, 93.1ms\n",
      "Speed: 1.3ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 potted plant, 1 bed, 1 mouse, 101.1ms\n",
      "Speed: 1.7ms preprocess, 101.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 207.0ms\n",
      "Speed: 2.3ms preprocess, 207.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "\n",
      "0: 384x640 1 bed, 121.4ms\n",
      "Speed: 1.4ms preprocess, 121.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 145.1ms\n",
      "Speed: 1.4ms preprocess, 145.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 125.6ms\n",
      "Speed: 2.2ms preprocess, 125.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 88.2ms\n",
      "Speed: 1.7ms preprocess, 88.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 91.9ms\n",
      "Speed: 2.8ms preprocess, 91.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 105.1ms\n",
      "Speed: 1.6ms preprocess, 105.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 173.3ms\n",
      "Speed: 2.3ms preprocess, 173.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 146.2ms\n",
      "Speed: 1.5ms preprocess, 146.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 96.1ms\n",
      "Speed: 1.7ms preprocess, 96.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 99.3ms\n",
      "Speed: 1.4ms preprocess, 99.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 126.8ms\n",
      "Speed: 1.3ms preprocess, 126.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 103.9ms\n",
      "Speed: 2.1ms preprocess, 103.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 98.6ms\n",
      "Speed: 1.3ms preprocess, 98.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 115.5ms\n",
      "Speed: 1.4ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 93.3ms\n",
      "Speed: 1.6ms preprocess, 93.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 98.5ms\n",
      "Speed: 1.7ms preprocess, 98.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 112.6ms\n",
      "Speed: 1.8ms preprocess, 112.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 111.0ms\n",
      "Speed: 1.8ms preprocess, 111.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 109.9ms\n",
      "Speed: 2.2ms preprocess, 109.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 111.5ms\n",
      "Speed: 1.6ms preprocess, 111.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 106.1ms\n",
      "Speed: 2.2ms preprocess, 106.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 120.9ms\n",
      "Speed: 2.7ms preprocess, 120.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 155.1ms\n",
      "Speed: 1.7ms preprocess, 155.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 106.6ms\n",
      "Speed: 1.6ms preprocess, 106.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 144.7ms\n",
      "Speed: 3.7ms preprocess, 144.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "0: 384x640 1 bed, 103.7ms\n",
      "Speed: 3.3ms preprocess, 103.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "0: 384x640 1 bed, 205.9ms\n",
      "Speed: 2.6ms preprocess, 205.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 132.4ms\n",
      "Speed: 1.5ms preprocess, 132.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 110.5ms\n",
      "Speed: 1.3ms preprocess, 110.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 98.6ms\n",
      "Speed: 2.9ms preprocess, 98.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 117.1ms\n",
      "Speed: 1.7ms preprocess, 117.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 109.2ms\n",
      "Speed: 1.4ms preprocess, 109.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 118.2ms\n",
      "Speed: 16.2ms preprocess, 118.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 103.9ms\n",
      "Speed: 1.6ms preprocess, 103.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 cup, 1 bed, 101.4ms\n",
      "Speed: 1.4ms preprocess, 101.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 138.1ms\n",
      "Speed: 1.6ms preprocess, 138.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 141.8ms\n",
      "Speed: 1.5ms preprocess, 141.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 cup, 1 bed, 142.6ms\n",
      "Speed: 2.6ms preprocess, 142.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 153.9ms\n",
      "Speed: 1.7ms preprocess, 153.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 102.4ms\n",
      "Speed: 1.7ms preprocess, 102.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "0: 384x640 1 bed, 97.3ms\n",
      "Speed: 9.2ms preprocess, 97.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 100.0ms\n",
      "Speed: 1.6ms preprocess, 100.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 103.4ms\n",
      "Speed: 1.7ms preprocess, 103.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 92.3ms\n",
      "Speed: 3.6ms preprocess, 92.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 116.7ms\n",
      "Speed: 1.3ms preprocess, 116.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 130.6ms\n",
      "Speed: 1.8ms preprocess, 130.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 97.7ms\n",
      "Speed: 2.2ms preprocess, 97.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "0: 384x640 1 bed, 109.5ms\n",
      "Speed: 1.6ms preprocess, 109.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 104.7ms\n",
      "Speed: 1.7ms preprocess, 104.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 111.3ms\n",
      "Speed: 1.7ms preprocess, 111.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 105.0ms\n",
      "Speed: 1.6ms preprocess, 105.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bed, 135.8ms\n",
      "Speed: 1.4ms preprocess, 135.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 83.7ms\n",
      "Speed: 1.7ms preprocess, 83.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 131.8ms\n",
      "Speed: 1.2ms preprocess, 131.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "0: 384x640 1 bottle, 1 bed, 707.5ms\n",
      "Speed: 1.5ms preprocess, 707.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "\n",
      "0: 384x640 1 bed, 107.7ms\n",
      "Speed: 1.5ms preprocess, 107.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 125.0ms\n",
      "Speed: 1.4ms preprocess, 125.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bottle, 1 bed, 121.0ms\n",
      "Speed: 1.9ms preprocess, 121.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 111.2ms\n",
      "Speed: 2.1ms preprocess, 111.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 111.0ms\n",
      "Speed: 2.2ms preprocess, 111.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 144.4ms\n",
      "Speed: 2.5ms preprocess, 144.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 102.5ms\n",
      "Speed: 1.4ms preprocess, 102.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "\n",
      "0: 384x640 1 bed, 133.8ms\n",
      "Speed: 2.5ms preprocess, 133.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 116.6ms\n",
      "Speed: 2.3ms preprocess, 116.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 115.4ms\n",
      "Speed: 1.7ms preprocess, 115.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 206.0ms\n",
      "Speed: 2.5ms preprocess, 206.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "\n",
      "0: 384x640 1 bed, 142.8ms\n",
      "Speed: 1.5ms preprocess, 142.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 cup, 1 bed, 99.5ms\n",
      "Speed: 1.4ms preprocess, 99.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 125.8ms\n",
      "Speed: 1.7ms preprocess, 125.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 109.5ms\n",
      "Speed: 1.7ms preprocess, 109.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "0: 384x640 1 bed, 119.6ms\n",
      "Speed: 1.7ms preprocess, 119.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 117.4ms\n",
      "Speed: 2.8ms preprocess, 117.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "0: 384x640 1 bed, 234.2ms\n",
      "Speed: 2.1ms preprocess, 234.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "0: 384x640 1 bed, 95.0ms\n",
      "Speed: 1.7ms preprocess, 95.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "0: 384x640 1 bed, 111.5ms\n",
      "Speed: 2.6ms preprocess, 111.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxkucher/pytorch/tour-de-room/detector.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/pytorch/tour-de-room/detector.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m             value \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/pytorch/tour-de-room/detector.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     res_arr\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maxkucher/pytorch/tour-de-room/detector.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m res \u001b[39m=\u001b[39m analyzer_model\u001b[39m.\u001b[39;49mpredict([res_arr])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/pytorch/tour-de-room/detector.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m room_name \u001b[39m=\u001b[39m labels[np\u001b[39m.\u001b[39margmax(res)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/pytorch/tour-de-room/detector.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m cv2\u001b[39m.\u001b[39mputText(frame, room_name, (\u001b[39m400\u001b[39m, \u001b[39m60\u001b[39m), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m2\u001b[39m, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:2620\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2611\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2612\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2613\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2614\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2617\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2618\u001b[0m         )\n\u001b[0;32m-> 2620\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2621\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2622\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2623\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2624\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2625\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2626\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2627\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2628\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2629\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2630\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2631\u001b[0m )\n\u001b[1;32m   2633\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1687\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1293\u001b[0m     x,\n\u001b[1;32m   1294\u001b[0m     y,\n\u001b[1;32m   1295\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1296\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1297\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1298\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1299\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1300\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1301\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1302\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1303\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1304\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/data_adapter.py:723\u001b[0m, in \u001b[0;36mListsOfScalarsDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m     sample_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(sample_weights)\n\u001b[1;32m    719\u001b[0m sample_weight_modes \u001b[39m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    720\u001b[0m     sample_weights, sample_weight_modes\n\u001b[1;32m    721\u001b[0m )\n\u001b[0;32m--> 723\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_adapter \u001b[39m=\u001b[39m TensorLikeDataAdapter(\n\u001b[1;32m    724\u001b[0m     x,\n\u001b[1;32m    725\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    726\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    727\u001b[0m     sample_weight_modes\u001b[39m=\u001b[39;49msample_weight_modes,\n\u001b[1;32m    728\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    729\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    730\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    731\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/data_adapter.py:314\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m indices\n\u001b[1;32m    309\u001b[0m \u001b[39m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# performance.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mmap(permutation)\u001b[39m.\u001b[39mprefetch(\u001b[39m1\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    317\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[39m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m      A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/dataset_ops.py:2280\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2278\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2280\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[1;32m   2281\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2282\u001b[0m     map_func,\n\u001b[1;32m   2283\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m   2284\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2285\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m debug_mode\u001b[39m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality \u001b[39m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m    108\u001b[0m     map_func,\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m    110\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m    111\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m    113\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    266\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1227\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1226\u001b[0m   \u001b[39m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1228\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1197\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1197\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m   1198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1201\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    692\u001b[0m     tracing_compilation\u001b[39m.\u001b[39mScopeType\u001b[39m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    693\u001b[0m )\n\u001b[1;32m    694\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mtrace_function(\n\u001b[1;32m    696\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[1;32m    697\u001b[0m )\n\u001b[1;32m    699\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    700\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[39m=\u001b[39m tracing_options\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[39m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tracing_options\u001b[39m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[39m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[39m=\u001b[39m _create_concrete_function(\n\u001b[1;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m tracing_options\u001b[39m.\u001b[39mfunction_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[39m.\u001b[39mfunction_cache\u001b[39m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[39m=\u001b[39m function_type\u001b[39m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[39m=\u001b[39m tracing_options\u001b[39m.\u001b[39mattributes \u001b[39mand\u001b[39;00m tracing_options\u001b[39m.\u001b[39mattributes\u001b[39m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[39m.\u001b[39mDISABLE_ACD, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    311\u001b[0m     tracing_options\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    312\u001b[0m     tracing_options\u001b[39m.\u001b[39;49mpython_function,\n\u001b[1;32m    313\u001b[0m     placeholder_bound_args\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m    314\u001b[0m     placeholder_bound_args\u001b[39m.\u001b[39;49mkwargs,\n\u001b[1;32m    315\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    316\u001b[0m     func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    317\u001b[0m     add_control_dependencies\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m disable_acd,\n\u001b[1;32m    318\u001b[0m     arg_names\u001b[39m=\u001b[39;49mfunction_type_utils\u001b[39m.\u001b[39;49mto_arg_names(function_type),\n\u001b[1;32m    319\u001b[0m     create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m transform\u001b[39m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[39m=\u001b[39m traced_func_graph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/func_graph.py:1025\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1016\u001b[0m func_graph\u001b[39m.\u001b[39mstructured_input_signature \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m     convert_structure_to_signature(\n\u001b[1;32m   1018\u001b[0m         func_args, arg_names, signature_context\u001b[39m=\u001b[39msignature_context),\n\u001b[1;32m   1019\u001b[0m     convert_structure_to_signature(\n\u001b[1;32m   1020\u001b[0m         func_kwargs, signature_context\u001b[39m=\u001b[39msignature_context))\n\u001b[1;32m   1022\u001b[0m \u001b[39m# Note: `nest.flatten` sorts by keys, as does `_deterministic_dict_values`.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[39m# Variables to help check whether mutation happens in calling the function\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[39m# Copy the recursive list, tuple and map structure, but not base objects\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m func_args_before \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mpack_sequence_as(\n\u001b[1;32m   1026\u001b[0m     func_args,\n\u001b[1;32m   1027\u001b[0m     nest\u001b[39m.\u001b[39;49mflatten(func_args, expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m   1028\u001b[0m     expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1029\u001b[0m func_kwargs_before \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   1030\u001b[0m     func_kwargs,\n\u001b[1;32m   1031\u001b[0m     nest\u001b[39m.\u001b[39mflatten(func_kwargs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m   1032\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1034\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(x):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/nest.py:540\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.pack_sequence_as\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpack_sequence_as\u001b[39m(structure, flat_sequence, expand_composites\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    428\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a given flattened sequence packed into a given structure.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m    TypeError: `structure` is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39;49mpack_sequence_as(\n\u001b[1;32m    541\u001b[0m       nest_util\u001b[39m.\u001b[39;49mModality\u001b[39m.\u001b[39;49mCORE, structure, flat_sequence, expand_composites\n\u001b[1;32m    542\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/nest_util.py:856\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(modality, structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a given flattened sequence packed into a given structure.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \n\u001b[1;32m    735\u001b[0m \u001b[39m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[39m  non-sortable keys.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m--> 856\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m    857\u001b[0m       structure, flat_sequence, expand_composites, sequence_fn\n\u001b[1;32m    858\u001b[0m   )\n\u001b[1;32m    859\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m    860\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_pack_sequence_as(structure, flat_sequence)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/nest_util.py:920\u001b[0m, in \u001b[0;36m_tf_core_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(flat_structure) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(flat_sequence):\n\u001b[1;32m    914\u001b[0m     \u001b[39m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not pack sequence. Structure had \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m atoms, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mflat_sequence had \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m items.  Structure: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, flat_sequence: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(flat_structure), \u001b[39mlen\u001b[39m(flat_sequence), structure, flat_sequence)\n\u001b[1;32m    919\u001b[0m     )\n\u001b[0;32m--> 920\u001b[0m \u001b[39mreturn\u001b[39;00m sequence_fn(structure, packed)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/nest_util.py:225\u001b[0m, in \u001b[0;36msequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39melif\u001b[39;00m _is_mapping_view(instance):\n\u001b[1;32m    223\u001b[0m   \u001b[39m# We can't directly construct mapping views, so we create a list instead\u001b[39;00m\n\u001b[1;32m    224\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(args)\n\u001b[0;32m--> 225\u001b[0m \u001b[39melif\u001b[39;00m is_namedtuple(instance) \u001b[39mor\u001b[39;00m _is_attrs(instance):\n\u001b[1;32m    226\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(instance, _wrapt\u001b[39m.\u001b[39mObjectProxy):\n\u001b[1;32m    227\u001b[0m     instance_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(instance\u001b[39m.\u001b[39m__wrapped__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/nest_util.py:172\u001b[0m, in \u001b[0;36mis_namedtuple\u001b[0;34m(instance, strict)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_namedtuple\u001b[39m(instance, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    161\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns True iff `instance` is a `namedtuple`.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m    True if `instance` is a `namedtuple`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m   \u001b[39mreturn\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39;49mIsNamedtuple(instance, strict)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "video_file = \"/Users/maxkucher/pytorch/tour-de-room/video.mp4\"\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "model = YOLO(\"/Users/maxkucher/pytorch/yolov8n.pt\")\n",
    "\n",
    "names = model.names\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    table_value = 0\n",
    "    toilet_value = 0\n",
    "    bed_value = 0\n",
    "    chair_value = 0\n",
    "    tv_value = 0\n",
    "    microwave_value = 0\n",
    "    oven_value = 0\n",
    "    laptop_value = 0\n",
    "    couch_value = 0\n",
    "\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    results = model(frame)[0]\n",
    "\n",
    "    cv2.putText(frame, f\"Room type: \", (5, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "\n",
    "    for result in results.boxes.data.tolist():\n",
    "        features = [\"table\", \"toilet\", \"bed\", \"chair\", \"tv\", \"microwave\", \"oven\", \"laptop\", \"couch\"] \n",
    "        features_values = [table_value, toilet_value, bed_value, chair_value, tv_value, microwave_value, oven_value, laptop_value, couch_value] \n",
    "\n",
    "        labels = [\"kitchen\", \"bedroom\", \"living room\", \"schoolroom\", \"bathroom\"]\n",
    "\n",
    "\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "        name = results.names[int(class_id)] \n",
    "\n",
    "       \n",
    "        if score > threshold and name in features:\n",
    "            \n",
    "            x_center = int((x1 + x2) / 2)\n",
    "            y_center = int((y1 + y2) / 2)\n",
    "\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 4)\n",
    "            # draw a dot in the middle of detected object\n",
    "            cv2.circle(frame, (x_center, y_center), 5, (0, 0, 255), thickness=cv2.FILLED)\n",
    "            # put text under the box\n",
    "            cv2.putText(frame, name.upper(), (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            res_arr = []\n",
    "            for feature in features:\n",
    "                for value in features_values:\n",
    "                    if name == feature:\n",
    "                        value = 1\n",
    "                    else:\n",
    "                        value = 0\n",
    "\n",
    "                res_arr.append(value)\n",
    "\n",
    "        \n",
    "\n",
    "            res = analyzer_model.predict([res_arr])\n",
    "            room_name = labels[np.argmax(res)]\n",
    "\n",
    "            cv2.putText(frame, room_name, (400, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    #       \n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features:\n",
    "dining table, toilet, bed, chair, tv, microwave, oven, laptop, couch, \n",
    "\n",
    "labels:\n",
    "kitchen, bedroom, living room, schoolroom, bathroom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
